import os
import time

from selenium import webdriver
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import NoSuchElementException
from selenium.common.exceptions import TimeoutException as TS
from selenium.webdriver.support import expected_conditions as conditions
from selenium.webdriver.common.by import By
import pickle as pk
import requests


# def url_builder(root_url, page_number):
#     url = f"{root_url}/page/{page_number}"
#     return url


def fetch_links(driver, url):
    links = []
    print(f"Fetching job links from {url}")
    driver.get(url)

    # Clicking on the view all element to allow all links to be loaded in the webpage
    driver.find_element(
        By.XPATH,
        "/html/body/div/div/div/div/div/div[2]/main/div/div[2]/form[2]/select/option[8]",
    ).click()
    x = 1  # Starting value of x
    skip_counter = 0

    while x < 1001:
        try:
            # Construct the XPath for the div with the current x value
            xpath = f"/html/body/div/div/div/div/div/div[2]/main/ul/li[{x}]/a"

            # try to get the element specified by the xpath, if it does not exist skip it
            try:
                WebDriverWait(driver, 1).until(
                    conditions.presence_of_element_located((By.XPATH, xpath))
                )
            except TS:
                x += 1
                skip_counter += 1
                continue

            element = driver.find_element_by_xpath(xpath)
            try:
                # Find and store the first link within the div, if it exists
                links.append(element.get_attribute("href"))

            except NoSuchElementException:
                pass

            x += 1  # Increment x for the next iteration

        except NoSuchElementException:
            # When there are no more divs with the current x value, exit the loop
            break

    # pagination = check_pagination(driver)

    # if pagination > 0:
    #     print(f"Found {pagination} paginated pages, adding them to queue")
    #     sub_urls = [url]
    #     for page_number in range(2, pagination + 1):
    #         sub_urls.append(url_builder(url, page_number))

    #     for url in sub_urls:
    #         scraper_logic(url)

    return links


def check_pagination(driver):
    # returns number of pages if pagination exits or false if none

    try:
        xpath = '//*[@id="main"]/nav/ul'

        WebDriverWait(driver, 1).until(
            conditions.presence_of_element_located((By.XPATH, xpath))
        )
        pagination = driver.find_element(By.XPATH, xpath)

        pages = pagination.find_elements_by_tag_name("li")

        last_page = int(pages[-2].text)

        return last_page
    except TS:
        return False


def store_links(links):
    filename = "Outputs/links/codingkenya.pk"
    try:
        file = open(filename, "wb")
    except FileNotFoundError:
        print("Links folder not found, creating one.")
        os.mkdir("Outputs/links/")
        file = open(filename, "wb")

    pk.dump(links, file)
    file.close()

    print(
        f"Done. {len(links)} links from CodingJobs Kenya have been fetched and pickledðŸ«™"
    )


def main():
    options = Options()
    options.add_argument("--headless")
    url = "https://codingkenya.com/jobs"
    links = []

    #           categories = [
    #         "artificial-intelligence",
    #         "automation",
    #         "backend-engineer",
    #         "big-data",
    #         "blockchain-developer",
    #         "cloud-solutions-architect",
    #         "data-engineer",
    #         "data-officer",
    #         "database-administrator",
    #         "coding",
    #         "cybersecurity",
    #         "devops-engineer",
    #         "digital-complience-and-audit",
    #         "digital-learning-specialist",
    #         "front-end-developer",
    #         "front-end-engineer",
    #         "fullstack-engineer",
    #         "full-stack-web-developer",
    #         "graphic-designer",
    #         "ict-officer",
    #         "helpdesk-support-analyst",
    #         "internet-of-things",
    #         "internship",
    #         "it-administrator",
    #         "it-consulting",
    #         "machine-learning",
    #         "software-developer",
    #         "software-engineer",
    #         "senior-software-engineer",
    #         "systems-administrator",
    #         "systems-developer",
    #         "ui-ux-designer",
    #         "web-developer",
    #     ]

    driver = webdriver.Chrome(options=options)

    # starts from page 2 because page 1 is already accounted for in the initial list

    tac = time.perf_counter()

    # Check if page exists/returns a 404
    response = requests.get(url)
    if response.status_code == 404:
        print("Link does not exist. Stopping...")
    else:
        links.append(fetch_links(driver, url))
        # print(fetch_links(driver, url))

    tic = time.perf_counter()

    # flattening the lists into one dimension
    links = [link for sublist in links for link in sublist]
    store_links(links)

    print(f"Time Taken:. {(tic - tac):.3f} seconds")

    # Closes driver agent
    driver.quit()


if __name__ == "__main__":
    main()
